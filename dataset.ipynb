{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88472d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb81b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(size:int):\n",
    "    mask = torch.triu(input=torch.ones((size,size)), diagonal=1).type(torch.int)\n",
    "    #We have an lower zero diag matrix but we want and upper one\n",
    "    return mask == 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac4a02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BillingualDataset(Dataset):\n",
    "    def __init__(self,ds,src_lang,tgt_lang,max_len ,src_tokenizer,tgt_tokenizer):\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "        self.sos_token = torch.tensor([src_tokenizer.token_to_id(\"SOS\")], dtype = torch.int64)\n",
    "        self.pad_token = torch.tensor([src_tokenizer.token_to_id(\"PAD\")] , dtype = torch.int64)\n",
    "        self.eos_token = torch.tensor([src_tokenizer.token_to_id(\"EOS\")] , dtype = torch.int64)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        src_lang = self.src_lang\n",
    "        tgt_lang = self.tgt_lang\n",
    "        src_tgt_pair = self.ds[index]\n",
    "        src_text = src_tgt_pair['translation'][src_lang]   \n",
    "        tgt_text = src_tgt_pair['translation'][tgt_lang]\n",
    "        \n",
    "        enc_input_tokens = self.src_tokenizer.encode(src_text).ids # [1,5,23,19] for example for one sentance : output is a list\n",
    "        dec_input_tokens = self.tgt_tokenizer.encode(src_text).ids # [1,5,23,19] for example for one sentance\n",
    "        \n",
    "        #Every sequence has a variable len and we want to have the same length for every sentance\n",
    "        enc_num_pad = self.max_len - len(enc_input_tokens) - 2 #We have the EOS and SOS tokens\n",
    "        dec_num_pad = self.max_len - len(enc_input_tokens) - 1 # We only have the SOS token for the decoder\n",
    "        \n",
    "         \n",
    "        if enc_num_pad < 0 or dec_num_pad <0 :\n",
    "            raise ValueError('Sentance is too long')\n",
    "\n",
    "        #Construct the final tensors\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "               self.sos_token,\n",
    "               torch.tensor([enc_input_tokens], dtype=torch.int64),\n",
    "               self.eos_token,\n",
    "               torch.tensor([self.pad_token] * enc_num_pad , dtype=torch.int64)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor([dec_input_tokens], dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_pad , dtype=torch.int64)\n",
    "\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor([dec_input_tokens], dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_pad , dtype=torch.int64)\n",
    "                \n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        assert encoder_input.size(0) == self.max_len \n",
    "        assert decoder_input.size(0) == self.max_len \n",
    "        assert label.size(0) == self.max_len\n",
    "        \n",
    "        return {\n",
    "            \"encoder_input\": encoder_input ,\n",
    "            \"deocder_input\": decoder_input,\n",
    "            \"label\":label,\n",
    "            # But i will also need a mask to ignore the PAD tokens during the attention mechanism\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1,1,max_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_txt\": tgt_text\n",
    "        }\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
